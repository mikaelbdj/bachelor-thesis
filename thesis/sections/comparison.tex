\documentclass[../master/master.tex]{subfiles}
\begin{document}
In this section, we now compare the results of experiments to each other and to the theoretical analysis of the algorithms and their complexity.

From the graphs above, the algorithms do not show a clear $\mathcal{O}(n)$ or $\mathcal{O}(n\log n)$ bound. However, we also know that there is a tighter bound of $\mathcal{O}(\text{min}(n, dN))$ on the linear-time algorithm and $\mathcal{O}(\text{min}(n\log n, dN)$ for the Lockstep algorithm, where $d$ is the diameter of the graph and $N$ is the number of SCCs, which still gives us a worst-case performance of $\mathcal{O}(n)$ and $\mathcal{O}(n\log n)$. This bound also explains why the number of SCCs has a positive correlation to the number of steps.

In the thesis, we decribe that edge restriction and trimming should improve the bound somewhat. In the graphs from our experiment, we can see that the different variations of the algorithms perform similarily to the originals. In the graph where we compare the vanilla and edge restriction Lockstep (Figure \ref{er graph}), we can see that edge restriction behaves quite randomly, so we cannot conclude that it always helps, as we hoped we could. It may help in some cases, however we have investigated it further in this thesis.

The second improvement we suggested was trimming. As can be seen in the different graphs, it does have an effect on the number of steps and the time taken. The algorithms with trimming perform either just as well as the vanilla versions, or better. When implementing trimming, we decided to only trim once at the very beginning. It is definitely worth considering how many times trimming should be run to optimize the algorithms, whether the amount of times it is run should depend on the type of graph it is run on and if it shoyld be run in between recursive calls.

The theoretical analysis of the complexity of the two algorithms suggests that the linear-time algorithm should perform better. Although we expected that the linear-time algorithm would perform better or just as well as Lockstep in the number of steps, we thought it would perform worse in terms of runtime, as Lockstep is used more often in practice\todo{cite this}. This was not the case. We were surprised to see that there was only a small difference in the number of steps performed on the differnt graphs by the two algorithms. However, what was more surprising was the fact that the Lockstep algorithm performed slightly better when only looking at the number of steps. When comparing the time it took to run the algorithms, it seemed that linear tended to perform better than Lockstep.

To get a better overview of how well the algorithms performed, we ranked the algorithms by calculating a weighted arithmetic mean. We deemed the larger graphs as more important, and therefore gave them a heavier weight in the calculations: $\sqrt[2]{nodes}$. The results of this weighted average can be seen in Figure \ref{mean}.

\begin{figure}[h]
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    Algorithm              & Time   & Steps  \\
    \hline
    Linear                 & 85.75  & 208.88 \\
    \hline
    Linear w/ trim         & 84.86  & 134.33 \\
    \hline
    Lockstep               & 145.54 & 121.45 \\
    \hline
    Lockstep w/ ER         & 91.26  & 121.45 \\
    \hline
    Lockstep w/ ER \& Trim & 78.17  & 74.89 \\
    \hline
\end{tabular}
\caption{results of the weighted arithmetic mean}\label{mean}
\end{figure}

From this data, we can see that the choice of algorithm should depend on whether one values the number of symbolic steps performed or the runtime of the algorithm. If time is of the essence, which is often the case in practice, the linear-time algorithm should be run - but if one wants to minimize the number of steps performed, Lockstep performs best on most graphs, as the worst-case $O(n\log n)$ bound is rarely reached. Furthermore, trimming should be used, as it can improve both number of symbolic operations performed and runtime, as can be seen by the number in Figure \ref{mean}. By looking at the graphs from the experiments, we did not expect that generally edge restriction does seem to improve the runtime slightly, however this shoudl be tested more, to see in which cases edge restriction is useful.

% Small graphs:
% \begin{itemize}
% \item not so much correlation between nodes and steps
% \item moderate positive correlation between nodes and time
% \item moderate positive correlation between number of SCCs in graph and steps - although with some outliers if there are many nodes in an SCC
% \item generally negative correaltion between how many nodes there are per scc and steps
% \item linears and locksteps tend to stick together, with trimming helping slightly some of the time. Linears generally tend to do a little worse than locksteps
% \item locksteps seem to perform better time wise, but worse step wise
% \end{itemize}
% Big graphs:
% \begin{itemize}
% \item slight positive correlation between nodes and steps if trimmign not used - trimming improves complexity
% \item slight positive, but really no correlation between time and nodes, trimming improves runtime
% \item positive correlation betwen no of SCCs and steps, trimming helps with bigger graphs
% \item generally negative correlation between nodes per scc and steps, where trimming helps significantly on some data
% \item linears perform slightly worse than locksteps in steps
% \item linears are slightly better on larger datasets time-wise
% \end{itemize}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../master/master"
%%% End:
