\documentclass[../master/master.tex]{subfiles}
\begin{document}
In this section, we now compare the results of experiments to each other and to the theoretical analysis of the algorithms and their complexity.

From the graphs above, the algorithms do not show a clear $\mathcal{O}(n)$ or $\mathcal{O}(n\log n)$ bound. However, we also know that there is a tighter bound of $\mathcal{O}(\text{min}(n, dN))$ on the linear-time algorithm and $\mathcal{O}(\text{min}(n\log n, dN)$ for the Lockstep algorithm, where $d$ is the diameter of the graph and $N$ is the number of SCCs, which still gives us a worst-case performance of $\mathcal{O}(n)$ and $\mathcal{O}(n\log n)$. This bound also explains why the number of SCCs has a positive correlation to the number of steps.

To get a better overview of how well the algorithms performed, we ranked the algorithms by calculating a weighted geometric mean on normalized values for both time and steps. We deemed the larger graphs as more important, and therefore gave them a heavier weight in the calculations: $\log_2(|V|)$. The results of this weighted average can be seen in Figure \ref{mean}.

\begin{figure}[h]
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    Algorithm              & Time   & Steps  \\
    \hline
    Linear                 & 0.355 & 0.999 \\
    \hline
    Linear w/ trim         & 0.319 & 0.555 \\
    \hline
    Lockstep               & 0.422 & 0.536 \\
    \hline
    Lockstep w/ ER         & 0.357 & 0.536 \\
    \hline
    Lockstep w/ ER \& Trim & 0.328 & 0.305 \\
    \hline
\end{tabular}
\caption{results of the weighted geometric mean}\label{mean}
\end{figure}

In the thesis, we suggest that edge restriction and trimming should improve the bound somewhat. In the graphs from our experiment, we can see that the different variations of the algorithms perform similarly to the originals. In Figure \ref{er graph}, we can see that edge restriction does help in some cases and in other cases does the opposite. Thus, we cannot conclude that it always helps, as we had hoped. However, the results in Figure \ref{mean} do suggest that edge restriction decreases the practical run-time in the average case. 

The second improvement we investigated was trimming. As can be seen in the different graphs, it does have an effect on the number of steps and the time taken. The algorithms with trimming perform either just as well as the vanilla versions, or better. This is also supported by the results in Figure \ref{mean}. When implementing trimming, we decided to only trim once at the very beginning. It is definitely worth considering how many times trimming should be run to optimize the algorithms, whether the amount of times it is run should depend on some heuristic, and if it should be run in between recursive calls.

Based on their time complexities we expected that the linear-time algorithm would perform better or just as well as Lockstep in the number of steps. This was not the case. There was only a small difference in the number of steps performed on the different graphs by the two algorithms; more surpringly, the Lockstep algorithm performed slightly better when only looking at the number of steps. This suggests some hidden constants in the $\mathcal{O}$-notation of linear is inflating its number of steps, or that the lockstep algorithm rarely reaches its upperbound.

When comparing the time it took to run the algorithms, linear tended to perform better than Lockstep - once again this is supported by the results in Figure \ref{mean}.

Finally, we observe in Figure \ref{mean} that in terms of practical running time the linear algorithm with trimming performs the best. This makes sense, as linear tended to perform better than lockstep on large graphs and trimming on average improves the practical runtime.

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../master/master"
%%% End:
