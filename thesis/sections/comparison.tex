\documentclass[../master/master.tex]{subfiles}
\begin{document}
In this section, we now compare the results of experiments to each other and to the theoretical analysis of the algorithms and their complexity.

From the graphs above, the algorithms do not show a clear $\mathcal{O}(n)$ or $\mathcal{O}(n\log(n))$. However, we also know that there is a tighter bound of $\mathcal{O}(min(n, dN)$ on the linear-time algorithm and $\mathcal{O}(min(n\log(n), dN)$ for the Lockstep algorithm, where $d$ is the diameter of the graph and $N$ is the number of SCCs, which still gives us a worst-case performance of $\mathcal{O}(n)$ and $\mathcal{O}(n\log(n))$. This bound also explains why the number of SCCs has a positive correlation to the number of steps.

In the thesis, we decribe that edge restriction and trimming should improve the bound somewhat. In the graphs from our experiment, we can see that the different variations of the algorithms perform similarily to the originals. Edge restriction does not seem to alter the number of steps in comparison to the original algorithms. In the graph where we compare the vanilla and edge restriction Lockstep \todo{make graph a figure?}, we can see that edge restriction can improve the time it takes per symbolic step, thereby improving the overall time to run the algorithm. However, this is not always the case, so one should consider when using edge restriction is useful.

The second improvement we suggested was trimming. As can be seen in the different graphs,it does have an effect on the number of steps and the time taken. The algorithms with trimming perform either just as well as the vanilla versions, or better. When implementing trimming, we decided to only trim once at the very beginning. It is definitelt worth considering how many time trimming should be run to optimize the algorithms and whether the amount of times it is run shoudl depend on the type of graph it is run on.


Even though theoretically it would be smart to use the linear-time algorithm, we can see that the Lockstep performs better than the linear-time one when considering steps. However, if we look at the runtime of the two algorithms, linear seems to perform better with larger graphs.

If one wants to optimize the number of steps, trimming should be used, as it can make significant improvements in some cases. However, one should also consider the time it takes to find all SCCs when using trimming - as it may also worsen the runtime of the algorithms slightly in some cases.

% Small graphs:
% \begin{itemize}
% \item not so much correlation between nodes and steps
% \item moderate positive correlation between nodes and time
% \item moderate positive correlation between number of SCCs in graph and steps - although with some outliers if there are many nodes in an SCC
% \item generally negative correaltion between how many nodes there are per scc and steps
% \item linears and locksteps tend to stick together, with trimming helping slightly some of the time. Linears generally tend to do a little worse than locksteps
% \item locksteps seem to perform better time wise, but worse step wise
% \end{itemize}
% Big graphs:
% \begin{itemize}
% \item slight positive correlation between nodes and steps if trimmign not used - trimming improves complexity
% \item slight positive, but really no correlation between time and nodes, trimming improves runtime
% \item positive correlation betwen no of SCCs and steps, trimming helps with bigger graphs
% \item generally negative correlation between nodes per scc and steps, where trimming helps significantly on some data
% \item linears perform slightly worse than locksteps in steps
% \item linears are slightly better on larger datasets time-wise
% \end{itemize}
Future work: investigate when edge restriction is useful, look at how many times trimming shoudl be run to make it the most effective
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../master/master"
%%% End:
