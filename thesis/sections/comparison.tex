\documentclass[../master/master.tex]{subfiles}
\begin{document}
In this section, we now compare the results of experiments to each other and to the theoretical analysis of the algorithms and their complexity.

From the graphs above, the algorithms do not show a clear $\mathcal{O}(n)$ or $\mathcal{O}(n\log n)$ bound. However, we also know that there is a tighter bound of $\mathcal{O}(min(n, dN))$ on the linear-time algorithm and $\mathcal{O}(\text{min}(n\log n, dN)$ for the Lockstep algorithm, where $d$ is the diameter of the graph and $N$ is the number of SCCs, which still gives us a worst-case performance of $\mathcal{O}(n)$ and $\mathcal{O}(n\log n)$. This bound also explains why the number of SCCs has a positive correlation to the number of steps.

In the thesis, we decribe that edge restriction and trimming should improve the bound somewhat. In the graphs from our experiment, we can see that the different variations of the algorithms perform similarily to the originals. Edge restriction does not seem to alter the number of steps in comparison to the original algorithms. In the graph where we compare the vanilla and edge restriction Lockstep (Figure \ref{er graph}), we can see that edge restriction can in some cases improve the time it takes per symbolic step, thereby improving the overall time to run the algorithm. However, this is not always the case, so one should consider when using edge restriction is useful.

The second improvement we suggested was trimming. As can be seen in the different graphs, it does have an effect on the number of steps and the time taken. The algorithms with trimming perform either just as well as the vanilla versions, or better. When implementing trimming, we decided to only trim once at the very beginning. It is definitely worth considering how many times trimming should be run to optimize the algorithms and whether the amount of times it is run should depend on the type of graph it is run on.

The theoretical analysis of the complexity of the two algorithms suggests that the linear-time algorithm should perform better.  Although we expected that the linear-time algorithm would perform better or just as well as Lockstep in the number of steps, we thought it would perform worse in terms of runtime. This was not the case. We were surprised to see that there was only a small difference in the number of steps performed on the differnt graphs by the two algorithms. However, what was more surprising was the fact that the Lockstep algorithm performed slightly better when only looking at the number of steps. When comparing the time it took to run the algorithms, we found out that linear tended to perform better than Lockstep.

From the data we have gathered by running experiments, we can see that the choice of algorithm should depend on whether one values the number of symbolic steps performed or the runtime of the algorithm. If time is of the essence, which is often the case in practice, the linear-time algorithm should be run - but if one wants to minimize the number of steps performed, Lockstep performs best on most graphs, as the worst-case $O(n\log n)$ bound is rarely reached. Furthermore, trimming should be used, as it can improve both number of symbolic operations performed and runtime, or at least performs just as well as the vanilla versions of the algorithms. Edge restriction did not prove to be as useful as we thought. It did not have an impact on the number of symbolic steps, and it varied a lot from graph to graph whether it improved or worsened the runtime.

% Small graphs:
% \begin{itemize}
% \item not so much correlation between nodes and steps
% \item moderate positive correlation between nodes and time
% \item moderate positive correlation between number of SCCs in graph and steps - although with some outliers if there are many nodes in an SCC
% \item generally negative correaltion between how many nodes there are per scc and steps
% \item linears and locksteps tend to stick together, with trimming helping slightly some of the time. Linears generally tend to do a little worse than locksteps
% \item locksteps seem to perform better time wise, but worse step wise
% \end{itemize}
% Big graphs:
% \begin{itemize}
% \item slight positive correlation between nodes and steps if trimmign not used - trimming improves complexity
% \item slight positive, but really no correlation between time and nodes, trimming improves runtime
% \item positive correlation betwen no of SCCs and steps, trimming helps with bigger graphs
% \item generally negative correlation between nodes per scc and steps, where trimming helps significantly on some data
% \item linears perform slightly worse than locksteps in steps
% \item linears are slightly better on larger datasets time-wise
% \end{itemize}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../master/master"
%%% End:
