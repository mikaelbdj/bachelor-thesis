\documentclass[../master/master.tex]{subfiles}
\begin{document}
We have analysed, implemented and experimented upon the two algorithms by Bloem et. al. \cite{linear} and by Gentilini et. al. \cite{lockstep} for decomposion of a graph
$G$ into its strongly connected components. We provide a tighter theoretical analysis of its complexity in terms of the symbolic steps used and parameterized by the diameter of the input graph. Asymptotically speaking the \textit{Linear} algorithm performs better than the \textit{Lockstep} algorithm for large enough input graphs. However, our experiments show that in practice the \textit{Lockstep} algorithm performed slightly better in the average case. This must either be due to that for graphs of a few million nodes or less hidden constants in the $\mathcal{O}$-notation of \textit{Linear} outweighs its better worst-case performance, or that \textit{Lockstep} simply does not come close to its upperbounded complexity in the average case. If one has to choose between the vanilla versions, the \textit{Linear} algorithm should be chosen as it performed best time-wise - increasingly so on larger graphs.

We also investigated two possible optimisations of the algorithms: \textit{trimming} and \textit{edge restriction}. When adding trimming onto \textit{Linear} the performance only increases. Also here, \textit{Linear} with trimming performs better than all other algorithms (though only slightly better than \textit{Lockstep} with trimming and edge restriction). Edge restriction did show to have a positive effect in the average case, and thus it should be considered a good improvement to the vanilla \textit{Lockstep} algorithm. However, we did see cases where edge restriction worsened the runtime/steps. Trimming showed an even better improvement - and contrary to edge restriction it never worsened the runtime. This heuristic should definitely be applied in practice on both algorithms.

\subsection{Future work}
While we did not have the chance to look at edge restriction and trimming in more depth while writing this thesis, we think it would be very interesting to look into this further.

The effect of edge restriction tended to vary between improving the runtime or slowing it down. While our data suggests it would be worth using in the average case, we think it would be advantageous to investigate it in more depth, perhaps looking at the graphs it performed better on. It is possible that edge restriction might be more effective on certain kinds of graphs - perhaps graphs below or above a certain diameter, some number of SCC, the ratio of nodes to edges or the number of BDD nodes.

Trimming on the other hand proved to be very useful, and using it on all graphs would be wise, as it does not harm the performace of the algorithms, and can improve it. What we did notice was that sometimes only running trimming once did not affect the number of symbolic steps or time at all. This is due to the fact that it removes a very small percentage of nodes from the graphs, which does not help the overall performance. The effect of trimming on symbolic steps and time may also vary depending on how interconnected the graph is. It would be worth looking at how many times trimming should be run to optimize its effectiveness. An interesting heuristic to look at would be to continue trimming until trimming only removes a small percentage of nodes; as then it starts being less effective and may take a toll on the overall runtime if done too many times. One could also choose to look at whether performing trimming in between each iteration of the algorithms would further improve the runtime.
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../master/master"
%%% End:
